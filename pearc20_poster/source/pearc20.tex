% !TeX spellcheck = en_GB
%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[manuscript,screen]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
	\providecommand\BibTeX{{%
			\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}

\copyrightyear{2020}
\acmYear{2020}
\setcopyright{acmcopyright}\acmConference[PEARC '20]{Practice and Experience in Advanced Research Computing}{July 26--30, 2020}{Portland, OR, USA}
\acmBooktitle{Practice and Experience in Advanced Research Computing (PEARC '20), July 26--30, 2020, Portland, OR, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3311790.3399615}
\acmISBN{978-1-4503-6689-2/20/07}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Benchmark informed software upgrades on Quest, Northwestern's HPC cluster}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Sajid Ali}
\orcid{0000-0003-2186-4636}
\affiliation{%
  \institution{Applied Physics, Northwestern University}
  \streetaddress{2145 Sheridan Road}
  \city{Evanston}
  \state{Illinois}
  \postcode{60208}
}
\email{sajidsyed2021@u.northwestern.edu}

\author{Alex Mamach}
\affiliation{%
	\institution{NUIT, Northwestern University}
	\streetaddress{2145 Sheridan Road}
	\city{Evanston}
	\state{Illinois}
	\postcode{60208}
}
\email{alex.mamach@northwestern.edu}


\author{Alper Kinaci}
\affiliation{%
\institution{NUIT, Northwestern University}
  \streetaddress{2145 Sheridan Road}
  \city{Evanston}
  \state{Illinois}
  \postcode{60208}
  }
\email{akinaci@northwestern.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  We present the work performed at Quest, a high performance computing cluster at Northwestern University regarding benchmarking of software performed to guide software upgrades. We performed extensive evaluation of all MPI modules present on the system for functionality and performance in addition to testing a strategy to deploy architecture optimized software that can be loaded dynamically at runtime.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10011007.10011006.10011071</concept_id>
	<concept_desc>Software and its engineering~Software configuration management and version control systems</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10011007.10011006.10011073</concept_id>
	<concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
	<concept_significance>300</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software configuration management and version control systems}
\ccsdesc[300]{Software and its engineering~Software maintenance tools}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{software management, 	software builds, software automation}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Quest is a heterogeneous HPC cluster\cite{quest} at Northwestern University consisting of Intel Haswell/Broadwell/Skylake/Cascade Lake nodes with varying interconnects which recently deployed SLURM\cite{slurm} as the resource manager and the job scheduler. The cluster operates with very high uptimes and generally shuts down for a week once every academic year for maintenance. While this high uptime is great for research throughput, it compresses critical maintenance tasks into that week and makes the operators prioritize in place upgrades over major redesigns. While such an operations scheme works in the short run, managing a large set of software stacks that were installed at various points in time becomes challenging since the software stack was kept stable even through downtimes that involved major and minor OS upgrades.

This has led to a bloated software stack and inconsistent naming schemes for modules and executables. This is challenging to continuously benchmark for functionality and performance. Thus, we are motivated to develop a strategy to maintain our software stacks that will enable us to provide functional and efficient software for our users while reducing the maintenance and support workload for the operators and software specialists. In addition to the above, we also face an immediate need to make MPI launchers compatible with srun as a SLRUM update is on the agenda for next downtime (as part of the MPI upgrade project).

In this article, we present our ongoing project to modernize MPI installations and the results of benchmarking studies that inform our plans for deprecating old modules. We also discuss the preliminary tests on our beta cluster for a strategy to deploy optimized builds for each architecture that are dynamically loaded at runtime based upon the node list for the job.

\section{MPI libraries}
Over the 10-year life of Quest cluster, multiple versions of libraries proving functionality specified by the message passing interface standard \cite{mpi_3_1,mpi_2_2} were installed. Some of these are quite old (before major OS version upgrades) and the naming scheme for the corresponding module files is inconsistent. Thus, we developed the following strategy for an upgrade project
with the following milestones (in chronological order) :
\begin{itemize}
	\item[$\blacksquare$] Deployment of a beta cluster with SLURM 19 (later updated to SLURM 20)
	\item[$\blacksquare$] Benchmark existing MPI libraries on the beta cluster
	\item[$\blacksquare$] Benchmark new MPI libraries on beta cluster
	\item[$\blacksquare$] Deployment of updated middleware (UCX, PMIx) on the production cluster
	\item[$\blacksquare$] Communicate module deprecation and upgrade strategy to users
	\item[$\blacksquare$] User tests using recompiled or new MPI applications on beta cluster
	\item[$\blacksquare$] MPI modules transition along with SLURM update on production cluster
\end{itemize}

During the submission of this report, we are working on a user communication plan. Consequently we will primarily focus on tasks prior this milestone.

\subsection{Benchmarking}

To test these libraries for functionality and performance, we compiled and executed two point-to-point benchmarks, bandwidth and latency from the OSU micro-benchmarks suite \cite{osu_bench_website}. Bash scripts were used to loop over available libraries (with some requiring environment variables to be set to prevent errors) due to the fact that these MPI libraries were installed without SLURM support and had incostintent naming schemes. The results are presented in the figure \ref{fig:currmpi}.

\begin{figure}
	\hspace*{-1.5cm}\includegraphics[scale=0.6]{curr_pearc}
	\caption{Band of performance metrics measured for the currently
			 available MPI builds indicating a large spread in 
			 performance. Some builds fail to use the InfiniBand fabric while others had sub-optimal configurations. The list of libraries tested is listed in section A.1}
	\Description{Benchmark of currently available MPI builds}
	\label{fig:currmpi}
\end{figure}

In total, $42\%$ of the available MPI libraries were faulty with $28\%$ being nonfunctional (failure to compile or run) and $14\%$ being non-performant.

\subsection{Improvements}
 
Spack\cite{spack},a package manager that automates building complex software stacks was used to build new versions of MPI libraries with SLURM support. Spack allows us to automate a large set of parameterized builds and eases the testing. Alongside testing the new installations for srun launcher support, we also tested the relative performance of the UCX transport layer \cite{shamis2015ucx,openucx-website}. Unified Communication X (abbreviated as UCX) is a portable, high performance middleware that sits between programming models (like MPI, PGAS, charm++, etc) and network device drivers. The results of benchmarks with new installations are presented in the figure \ref{fig:newmpi}. 

\begin{figure}
	\hspace*{-1.5cm}\includegraphics[scale=0.6]{new_pearc}
	\caption{Band of performance metrics measured for the new MPI
		builds with optimal configuration and updated middleware
		indicating a reduced spread in performance with newer middleware providing some gains. The list of libraries tested is listed in section A.2}
	\Description{Benchmark of new MPI builds}
	\label{fig:newmpi}
\end{figure}

As indicated in a recent OpenMPI deployment guide \cite{openmpi_deployment_tuning}, newer versions of the libraries  configured with UCX transport layer perform better. We have thus decided to use the UCX transport layer for all MPI libraries. The newer versions of OpenMPI had $63\%$ lower latencies on average (more pronounced at smaller message sizes) and $66\%$ better bandwidth on avearge (more pronounced at intermediate message sizes). Configuring the newer versions of OpenMPI with UCX resulted in latency being reduced by $5\%$(more pronounced at larger message sizes) on avearge and increased bandwidth by $67\%$ on average (more pronounced at intermediate message sizes).

In addition to this, we also plan to enable the PMIx plugin \cite{slurm_pmix_sc17,slurm_pmix_2019} in SLURM to use the PMIx process management standard \cite{pmix,pmix_website} (implemented via the OpenPMIx library \cite{openpmix_website}) which improves the job startup time. We also note that the SLURM plugin for PMIx allows it optionally use UCX for communication via a SLURM plugin for UCX.

\subsection{Deploying UCX and PMIx}
While UCX was installed on the GPFS filesystem for convenience during testing, we plan to install it on the node-local filesystem (specifically at $/usr/local$) of each compute node given its nature as a widely used runtime dependency for MPI libraries. We already had an older version of UCX, $1.4.0$ available (as part of the Mellanox driver installations) and used this alongside an installation of OpenPMIx \cite{openpmix_website}, version $2.2.3$ for testing. Unfortunately, due to a bug in the SLURM plugin \cite{slurm_ucx_bug}, this configuration led to the job start phase crashing. The bug fix involves either installing UCX with the rdma-core \cite{rdmacore_repository}(which provide the user space components for the IB drivers) dependency or update the drivers to a newer version.

Since there are no plans on updating the InfiniBand drivers, we first attempted at manually creating binaries (in the $.rpm$ format using $rpm-builder$ tool) for UCX and PMIx that overcome the aforementioned bug. We had no success with this approach as we were unable to properly patch the libraries. Thus, we used Spack \cite{spack} to install the libraries to a common prefix (by using a filesystem view in an environment) and create an rpm binary from this for easy deployment on the compute nodes.

\section{Node arch dependent software}

Given the challenges in maintaining a complex software stack that includes a multitude of combinations between applications, versions, compilers and dependencies, achieving optimal software performance (as available via generating optimized binaries for each architecture) was not prioritized. While this was not a major concern in the past, increase in the CPU-architecture heterogeneity of the cluster in the recent years, such a deployment strategy severely degrades productivity. Thankfully, due to recent developments in the Spack package manager, we are able to build multiple versions of each library, each optimized for a different architecture for optimal productivity of the cluster. 

\subsection{Benchmarks}
We benchmarked optimized builds for two of our most commonly used applications, LAMMPS\cite{lammps} and GROMACS \cite{gromacs_1995,gromacs_2015} against the currently available installations on the oldest processor generation, "Haswell". We chose the Lennard-Jones liquid benchmark for LAMMPS available as part of the official benchmark suite \cite{lammps_bench} and a benchmark from the Unified European Applications Benchmark Suite \cite{ueabs_prace,ueabs_repo} for GROMACS. The results of these tests are presented in the Table \ref{tab:bench_apps} which shows that there are substantial benefits to deploying node architecture specific software even on our oldest nodes, with speedups of $32.3\%$ and $12.4\%$ observed for LAMMPS and GROMACS benchmark applications respectively.

\begin{table}
	\caption{Comparision of performance between currently available and architecture optimized builds of commonly used applications on Haswell nodes}
	\label{tab:bench_apps}
	\begin{tabular}{ccl}
		\toprule
		Software &Current&Optimized\\
		\midrule
		LAMMPS & 765.8 timesteps/day&1013.4 timesteps/day\\
		GROMACS & 1.78 ns/day&2.00 ns/day\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Deployment Strategy}
On Quest, users are not required to choose a partition and the jobs are assigned to nodes dynamically based on availability. Thus, we are faced with the following deployment challenge : how do we deploy optimized builds for each processor family but not require our users to choose the exact build of the application for their job ?

To answer the above, we have chosen a simple strategy where we configure a task prolog script for SLURM that automatically sets the modulepath based on $SLURM\_NODELIST$ environment variable. We tested this on a virtual SLURM cluster with two nodes on a laptop provisioned by four docker containers tied together via docker-compose using an existing repository \cite{slurmdocker_repository} developed by SciDAS (Scientific Data Analysis At Scale). This slurm cluster has two compute nodes named $worker01$ and $worker02$. We present the sample task prolog script below \footnote{The TaskProlog script is executed before the job starts and has the ability to set environment variables for a job.\cite{slurm_taskprolog}} : 

\begin{verbatim}
short_list=${SLURM_JOB_NODELIST##worker}
if [ $short_list == "01" ]
then
echo "export MODULEPATH=/home/path1"
fi
if [ $short_list == "02" ]
then
echo "export MODULEPATH=/home/path2"
fi
\end{verbatim}


\section{Conclusion}

We have effectively performed benchmarking tests that give us valuable insight regarding the health of software stack on the Northwestern-Quest high performance compute cluster. This work will inform our plans to deprecate and eventually remove non-functional and non-performant software. Moreover, executing the node-optimized software strategy will significantly improve the productivity of the cluster.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To various mailing lists, slack channels and forums including but not limited to mpich-discuss, slurm-info, spack-users.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{pearc20}

%%
%% If your work has an appendix, this is the place to put it.
\appendix
\section{Build details}
\subsection{Currently installed MPI libraries}
All the libraries available to the users were first built at a staging directory
for final installation at destination directory followed by modulefile creation. These include :
\begin{itemize}
	\item[$-$] intel-mpi-\{4.0.1,4.0.3,5.1.3.258\}
	\item[$-$] mpich-\{3.0.4-gcc-4.6.3, 3.0.4-gcc-4.8.3, 3.0.4-gcc-5.1.0, 	3.0.4-gcc-6.4.0, 3.0.4-intel2013.2, 3.0.4-intel2015.0,3.3-gcc-6.4.0\}
	\item[$-$]mvapich2-\{gcc-4.8.3, gcc-4.8.3-cuda8, intel2013.2\}
	\item[$-$]openmpi-1.6.3-\{gcc-4.6.3, gcc-4.8.3, intel2011.3, intel2013.2\}
	\item[$-$]openmpi-1.6.5-\{gcc-4.6.3, gcc-4.8.3, intel2013.2\}
	\item[$-$]openmpi-1.7.2-\{gcc-4.6.3, intel2013.2\}		\item[$-$]openmpi-1.8.1-\{gcc-4.6.3, intel2013.2\}
	\item[$-$]openmpi-1.8.3-\{gcc-4.8.3, gcc-4.8.3-mpi-threads, gcc-5.1.0, 	intel2013.2, intel2015.0\}
	\item[$-$]openmpi-1.8.6-\{gcc-4.8.3, gcc-4.8.3-debug, gcc-5.1.0, intel2013.2\}
	\item[$-$]openmpi-1.10.5-\{gcc-4.8.3, gcc-6.4.0, intel2013.2, intel2015.0, intel2016.0\}
	\item[$-$]openmpi-\{2.0.2-gcc-6.4.0-compute, 2.1.1-gcc-5.1.0, 2.1.2-intel2016.0\}
	\item[$-$]openmpi-\{3.0.0-intel2016.0, 3.1.3-gcc-6.4.0\}
\end{itemize} 
The general naming scheme is  $mpi-library-name/version/compiler-version/extra-flags$. In addition to our high uptimes, we rarely remove modules out of concerns over disruption to research. Following this strategy while upgrading the OS to a new major versions is the most common cause of non functional modules. These were compiled with older versions of system libraries but don't link with newer ones. The major cause of non performant builds, on the other hand, is errors in configuration of transport middleware and lack of benchmarking.

\subsection{New builds of MPI libraries}
As mentioned in the text, spack\cite{spack} was used to automate the builds which reduces time spent in building and allows us to shift our focus to testing. The newly built libraries include:
\begin{itemize}
	\item[$-$] openmpi-1.10.5-gcc-6.4.0 \{fabrics=verbs\}
	\item[$-$] openmpi-4.0.2-gcc-8.3.0 \{fabrics=verbs, fabrics=ucx\}
	\item[$-$] mpich-3.3.2-gcc-8.3.0 \{device=ch4, netmod=ucx\}
	\item[$-$] mpich-devl-gcc-8.3.0 \{device=ch4, netmod=ucx, pmi=pmix\}
	\item[$-$] mvapich-2.3.2-gcc-8.3.0 \{fabrics=mrail\}
\end{itemize} 
The newer library versions built with ucx in general perform better. The configurations are listed as per package conventions as represented by the corresponding variants in the spack package recipes.

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
